{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><b>Login Hugging Face</b></h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device set to: cuda\n",
      "Logged in to Hugging Face Hub successfully.\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import torch\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Device set to: {DEVICE}\")\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from huggingface_hub import login\n",
    "load_dotenv()\n",
    "HF_TOKEN = os.getenv(\"HF\")\n",
    "login(token=HF_TOKEN)\n",
    "print(\"Logged in to Hugging Face Hub successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><b>Settings</b></h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "FINETUNED_MODEL_PATH=\"data/model/Fine_Tuned_LLaMA2\"\n",
    "DATA_FILE_PATH = \"data/syntheses_10.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><b>Store Finatial Data (Evidence) To Vector Database</b></h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4><b>Prepare Functions</b></h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import chromadb\n",
    "from uuid import uuid4\n",
    "\n",
    "def get_data_to_store_in_chromaDB(file_path: str) -> pd.DataFrame:\n",
    "    # Load data from a CSV file\n",
    "    df = pd.read_csv(file_path)\n",
    "    df = df[['evidence_text']]\n",
    "    return df\n",
    "\n",
    "def store_to_chromaDB(df:pd.DataFrame, \n",
    "                      collection_name: str, \n",
    "                      database_path:str):\n",
    "    # Initialize the client\n",
    "    client = chromadb.PersistentClient(database_path)\n",
    "    \n",
    "    # Get or create the collection\n",
    "    collection = client.get_or_create_collection(name=collection_name)\n",
    "\n",
    "    # If the collection is empty, add the documents\n",
    "    if not collection.count():\n",
    "        for _, row in df.iterrows():\n",
    "            # Add document to collection with a unique ID\n",
    "            collection.add(\n",
    "                documents=row['evidence_text'],  # Ensure the column name is correct\n",
    "                ids=[str(uuid4())]  # Generate a unique ID using uuid4\n",
    "            )\n",
    "    print(f\"Financial Data added to collection '{collection_name}'.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4><b>Prepare Pipeline</b></h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_store_pipeline(source_data_path:str='data/syntheses_10.csv',\n",
    "                        collection_name: str=\"FinancialData\", \n",
    "                        database_path:str=\"data/vetorstore\"):\n",
    "    \n",
    "    data_to_store_in_chromaDB = get_data_to_store_in_chromaDB(file_path=source_data_path)\n",
    "    store_to_chromaDB(df=data_to_store_in_chromaDB,\n",
    "                      collection_name=collection_name,\n",
    "                      database_path=database_path)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5><b>Execute Pipeline</b></h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Financial Data added to collection 'FinancialData'.\n"
     ]
    }
   ],
   "source": [
    "data_store_pipeline(source_data_path='data/syntheses_10.csv',\n",
    "                                 collection_name=\"FinancialData\",\n",
    "                                 database_path=\"data/DB/vetorstore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><b>Load the Finetuned Model</b></h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4><b>Prepare Functions</b></h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from typing import Any\n",
    "\n",
    "def get_bnb_config(load_in_4bit: bool = True,\n",
    "                   bnb_4bit_use_double_quant: bool = True,\n",
    "                   bnb_4bit_quant_type: str = \"nf4\",\n",
    "                   bnb_4bit_compute_dtype: Any = torch.bfloat16\n",
    "                   ) -> BitsAndBytesConfig:\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=load_in_4bit,# Whether to load model in 4-bit precision\n",
    "        bnb_4bit_use_double_quant=bnb_4bit_use_double_quant, # Whether to use double quantization\n",
    "        bnb_4bit_quant_type=bnb_4bit_quant_type,# The quantization type (e.g., \"nf4\")\n",
    "        bnb_4bit_compute_dtype=bnb_4bit_compute_dtype# The compute dtype (e.g., torch.bfloat16, torch.float16)\n",
    "    )\n",
    "    return bnb_config\n",
    "\n",
    "def get_model(model_path: str,\n",
    "              bnb_config: BitsAndBytesConfig,\n",
    "              device:str):\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_path,\n",
    "                                                 quantization_config=bnb_config,\n",
    "                                                 device_map = \"cuda\")\n",
    "    return model\n",
    "\n",
    "\n",
    "def get_tokenizer(model_path: str, device: str) -> tuple:\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.padding_side = \"right\"\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4><b>Prepare Pipeline</b></h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_preparation_pipeline(model_path:str,\n",
    "                               device:str\n",
    "                                ):\n",
    "    \n",
    "    bnb_config = get_bnb_config()\n",
    "    model = get_model(model_path=model_path,\n",
    "                      bnb_config=bnb_config,\n",
    "                      device=device)\n",
    "    tokenizer = get_tokenizer(model_path=model_path,\n",
    "                              device=device)\n",
    "    \n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5><b>Execute Pipeline</b></h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.09s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got Finetuned Model.\n",
      "Got Finetuned Tokenizer.\n"
     ]
    }
   ],
   "source": [
    "finetuned_model, finetuned_tokenizer = model_preparation_pipeline(model_path=FINETUNED_MODEL_PATH,\n",
    "                                                                device=DEVICE)\n",
    "print(f\"Got Finetuned Model.\")\n",
    "print(f\"Got Finetuned Tokenizer.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><b>Generate Answer From LLM</b></h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4><b>Prepare Functions</b></h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file_path: str) -> pd.DataFrame:\n",
    "    # Load data from a CSV file\n",
    "    data = pd.read_csv(file_path)\n",
    "    if 'syntheses' in data.columns:\n",
    "        data.drop(\"syntheses\", axis=1, inplace=True)\n",
    "    return data\n",
    "\n",
    "def create_single_prompt(data_series: pd.Series) -> str:\n",
    "        \n",
    "        system_prompt = \"\"\"Give answer to questions provided below from the evidence text.\"\"\"\n",
    "        prompt_template = \"\"\"\n",
    "        <s>[INST]\n",
    "        <<SYS>>\n",
    "        {system_prompt}\n",
    "        <</SYS>>\n",
    "\n",
    "        Here is the question:\n",
    "        {question}\n",
    "\n",
    "        Consider the provided text as evidence:\n",
    "        {evidence_text}\n",
    "        [/INST]\n",
    "        \"\"\"\n",
    "        single_prompt = prompt_template.format(\n",
    "            system_prompt=system_prompt,  \n",
    "            question=data_series[\"question\"],  \n",
    "            evidence_text=data_series[\"evidence_text\"]  \n",
    "        )\n",
    "    \n",
    "        return single_prompt\n",
    "\n",
    "def generate(prompt, model, tokenizer, max_new_tokens: int = 100, device=\"cuda\"):\n",
    "    # Move model to the correct device\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Tokenize the input and move to the same device as the model\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    # Generate response\n",
    "    output = model.generate(input_ids=inputs[\"input_ids\"], max_new_tokens=max_new_tokens)\n",
    "\n",
    "    # Decode the response\n",
    "    generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "    return generated_text\n",
    "\n",
    "def extract_answer(generated_text):\n",
    "    # Extract the Answer Portion from the whole generated text\n",
    "    answer_start = generated_text.find(\"[/INST]\") + len(\"[/INST]\")  # Find the end of </INST> tag\n",
    "    answer = generated_text[answer_start:].strip()  # Extract everything after that position\n",
    "    return answer    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4><b>Prepare Pipeline</b></b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer_pipeline(singe_data_series,\n",
    "                             model,\n",
    "                             tokenizer):\n",
    "    single_prompt = create_single_prompt(singe_data_series)\n",
    "    generated_text = generate(single_prompt,model,tokenizer)\n",
    "    answer = extract_answer(generated_text)\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5><b>Execute Pipeline</b></h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = load_data(file_path=DATA_FILE_PATH)\n",
    "single_dataseries = dataframe.iloc[0]\n",
    "generated_answer = generate_answer_pipeline(single_dataseries,\n",
    "                                            finetuned_model,\n",
    "                                            finetuned_tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><b>Query with the Generated Answer in ChromaDB</b></h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4><b>Prepare Function</b></h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "\n",
    "def query_collection(generated_answer, \n",
    "                     collection_name=\"FinancialData\", \n",
    "                     database_path=\"data/DB/vetorstore\", \n",
    "                     n_results=5):\n",
    "    # Initialize the client\n",
    "    client = chromadb.PersistentClient(database_path)\n",
    "    \n",
    "    # Get or create the collection\n",
    "    collection = client.get_or_create_collection(name=collection_name)\n",
    "    \n",
    "    # Query the collection with the provided answer\n",
    "    query_results = collection.query(\n",
    "        query_texts=[generated_answer],  # Text to query\n",
    "        n_results=n_results  # Number of results to retrieve\n",
    "    )\n",
    "    \n",
    "    # Display the query results\n",
    "    documents = query_results.get('documents')\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5><b>Execute</b></h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#################### Result ####################\n",
      "\n",
      "['3M Company and Subsidiaries\\n Consolidated Statement of Income\\n Years ended December 31\\n (Millions, except per share amounts) 2022 2021 2020\\n Net sales $ 34,229 $ 35,355 $ 32,184___FINANCEBENCH_DELIMITER___3M Company and Subsidiaries\\n Consolidated Balance Sheet\\n At December 31\\n (Dollars in millions, except per share amount) 2022 2021 Property, plant and equipment — net 9,178 9,429 Total assets $ 46,455 $ 47,072___FINANCEBENCH_DELIMITER___3M Company and Subsidiaries\\n Consolidated Statement of Cash Flows\\n Years ended December 31\\n (Millions) 2022 2021 2020 Cash Flows from Investing Activities\\n Purchases of property, plant and equipment (PP&E) (1,749) (1,603) (1,501)', 'This marked the 65th consecutive year of dividend increases for 3M.', '3M Company and Subsidiaries\\nConsolidated Balance Sheet\\n(Unaudited)\\n(Dollars in millions, except per share amount) June 30, 2023 December 31, 2022\\nAssets\\nCurrent assets\\nCash and cash equivalents $ 4,258 $ 3,655\\nMarketable securities — current 56 238\\nAccounts receivable — net of allowances of $160 and $174 4,947 4,532\\nInventories\\nFinished goods 2,526 2,497\\nWork in process 1,527 1,606\\nRaw materials and supplies 1,227 1,269\\nTotal inventories 5,280 5,372\\nPrepaids 674 435\\nOther current assets 539 456\\nTotal current assets 15,754 14,688\\nProperty, plant and equipment 26,459 25,998\\nLess: Accumulated depreciation (17,248) (16,820)\\nProperty, plant and equipment — net 9,211 9,178\\nOperating lease right of use assets 812 829\\nGoodwill 12,869 12,790\\nIntangible assets — net 4,470 4,699\\nOther assets 5,764 4,271\\nTotal assets $ 48,880 $ 46,455\\nLiabilities\\nCurrent liabilities\\nShort-term borrowings and current portion of long-term debt $ 3,033 $ 1,938\\nAccounts payable 3,231 3,183\\nAccrued payroll 785 692\\nAccrued income taxes 172 259\\nOperating lease liabilities — current 244 261\\nOther current liabilities 3,471 3,190\\nTotal current liabilities 10,936 9,523', 'Balance Sheet\\nCash and cash equivalents at the end of the fourth quarter of fiscal 2022 were $737.9\\nmillion.\\nMerchandise inventories, net at the end of the fourth quarter of fiscal 2022 totaled $1.6\\nbillion compared to $1.5 billion at the end of the fourth quarter of fiscal 2021. The $104.2\\nmillion increase was primarily due to the opening of 47 new stores since January 29, 2022,\\ninventory to support new brand launches and brand expansions, and inventory cost\\nincreases.', 'The Boeing Company and Subsidiaries\\nNotes to the Consolidated Financial Statements\\nSummary of Business Segment Data\\n(Dollars in millions)\\nYears ended December 31, 2022 2021 2020\\nRevenues:\\nCommercial Airplanes $25,867 $19,493 $16,162\\nDefense, Space & Security 23,162 26,540 26,257\\nGlobal Services 17,611 16,328 15,543\\nBoeing Capital 199 272 261\\nUnallocated items, eliminations and other (231) (347) (65)\\nTotal revenues $66,608 $62,286 $58,158']\n",
      "\n",
      "*************************************************\n"
     ]
    }
   ],
   "source": [
    "query_results = query_collection(generated_answer)\n",
    "for query_result in query_results:\n",
    "    print(\"#################### Result ####################\")\n",
    "    print(f\"\\n{query_result}\\n\")\n",
    "    print(\"*************************************************\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
