{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "DATA_PATH = \"data/syntheses_10.csv\"  # Path to the dataset\n",
    "MODEL_PATH = \"NousResearch/Llama-2-7b-chat-hf\"\n",
    "FINETUNED_MODEL_PATH = \"data/model/Fine_Tuned_LLaMA2\"  # Path to the fine-tuned model output\n",
    "\n",
    "# Split parameters for dataset\n",
    "TEST_SIZE = 0.05  # Proportion of the dataset to include in the test split\n",
    "RANDOM_STATE = 42  # Random state for reproducibility\n",
    "\n",
    "# Prompts for training and prediction\n",
    "TRAINING_SYSTEM_PROMPT = \"\"\"You are a financial chatbot trained to answer questions based on the information provided.\n",
    "Your responses should be directly sourced from the content of the evidence_text(context).\n",
    "When asked a question, ensure that your answer is explicitly supported by the text and do not\n",
    "include any external information, interpretations, or assumptions not clearly stated in the evidence_text(context).\n",
    "If a question pertains to financial data or analysis that is not explicitly covered in the evidence_text(context) provided,\n",
    "respond by stating that the information is not available in the evidence_text(context).\n",
    "Your primary focus should be on accuracy, specificity, and adherence to the information in the evidence_text(context),\n",
    "particularly regarding financial statements, company performance, and market positions.\"\"\"\n",
    "\n",
    "TRAINING_PROMPT_TEMPLATE = \"\"\"\n",
    "<s>[INST]\n",
    "<<SYS>>\n",
    "{system_prompt}\n",
    "<</SYS>>\n",
    "{question}\n",
    "{evidence_text}\n",
    "[/INST]\n",
    "{answer}\n",
    "</s>\"\n",
    "\"\"\"\n",
    "\n",
    "PREDICTION_SYSTEM_PROMPT = \"\"\"Give answer to questions provided below from the evidence text.\"\"\"\n",
    "PREDICTION_PROMPT_TEMPLATE = \"\"\"\n",
    "<s>[INST]\n",
    "<<SYS>>\n",
    "{system_prompt}\n",
    "<</SYS>>\n",
    "\n",
    "Here is the question:\n",
    "{question}\n",
    "\n",
    "Consider the provided text as evidence:\n",
    "{evidence_text}\n",
    "[/INST]\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "# Training constants\n",
    "BATCH_SIZE = 4  # Number of samples per batch\n",
    "GRAD_ACCUM_STEPS = 4  # Gradient accumulation steps\n",
    "LEARNING_RATE = 2e-5  # Learning rate for optimization\n",
    "NUM_EPOCHS = 5  # Number of training epochs\n",
    "EVAL_STEPS = 50  # Evaluation interval in steps\n",
    "LOGGING_STEPS = 10  # Logging interval in steps\n",
    "MAX_SEQ_LENGTH = 100  # Maximum sequence length for input data\n",
    "\n",
    "#Prediction\n",
    "MAX_NEW_TOKENS = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device set to: cuda\n",
      "Logged in to Hugging Face Hub successfully.\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import torch\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Device set to: {DEVICE}\")\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from huggingface_hub import login\n",
    "load_dotenv()\n",
    "HF_TOKEN = os.getenv(\"HF\")\n",
    "login(token=HF_TOKEN)\n",
    "print(\"Logged in to Hugging Face Hub successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><b>Data Preparation</b></h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def load_data(file_path: str) -> pd.DataFrame:\n",
    "    # Load data from a CSV file\n",
    "    data = pd.read_csv(file_path)\n",
    "    if 'syntheses' in data.columns:\n",
    "        data.drop(\"syntheses\", axis=1, inplace=True)\n",
    "    return data\n",
    "\n",
    "def split_data(data: pd.DataFrame, test_size: float, random_state: int) -> tuple:\n",
    "    # Split the dataset into train and test sets\n",
    "    train_data, test_data = train_test_split(data, test_size=test_size, random_state=random_state)\n",
    "    return train_data, test_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_preparation_pipeline(data_file_path:str,test_size:float,random_state:int):\n",
    "    dataframe = load_data(file_path=data_file_path)\n",
    "    train_dataframe, test_dataframe = split_data(data=dataframe,test_size=test_size,random_state=random_state)\n",
    "    return train_dataframe,test_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dataframe Shape = (142, 3).\n",
      "Test Dataframe Shape = (8, 3).\n"
     ]
    }
   ],
   "source": [
    "train_dataframe, test_dataframe = data_preparation_pipeline(data_file_path=DATA_PATH,\n",
    "                                                          test_size=TEST_SIZE,\n",
    "                                                          random_state=RANDOM_STATE)\n",
    "\n",
    "print(f\"Train Dataframe Shape = {train_dataframe.shape}.\")\n",
    "print(f\"Test Dataframe Shape = {test_dataframe.shape}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><b>Prompt Preparation</b></h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def create_single_prompt(data_series: pd.Series, \n",
    "                         prompt_template: str, \n",
    "                         system_prompt: str, \n",
    "                         is_predict: bool = False) -> str:\n",
    "    single_prompt = \"\"  # Initialize an empty string to store the modified single prompt\n",
    "    \n",
    "    if is_predict:\n",
    "        # If is_predict is True, format the prompt without the answer\n",
    "        single_prompt = prompt_template.format(\n",
    "            system_prompt=system_prompt,  \n",
    "            question=data_series[\"question\"],  \n",
    "            evidence_text=data_series[\"evidence_text\"]  \n",
    "        )\n",
    "    else:\n",
    "        # If is_predict is False, format the prompt with the answer\n",
    "        single_prompt = prompt_template.format(\n",
    "            system_prompt=system_prompt,  \n",
    "            question=data_series[\"question\"],  \n",
    "            evidence_text=data_series[\"evidence_text\"],  \n",
    "            answer=data_series[\"answer\"]  \n",
    "        )\n",
    "    \n",
    "    return single_prompt\n",
    "\n",
    "def create_prompts(dataframe: pd.DataFrame,  \n",
    "                   prompt_template: str,  \n",
    "                   system_prompt: str,  \n",
    "                   is_predict: bool = False) -> list[str]:\n",
    "    prompts = []  # Initialize an empty list to store the generated prompts\n",
    "    \n",
    "    for _, row in dataframe.iterrows():  # Iterate over each row in the DataFrame\n",
    "        # Generate a single prompt\n",
    "        single_prompt = create_single_prompt(data_series=row,  \n",
    "                                             prompt_template=prompt_template,  \n",
    "                                             system_prompt=system_prompt,  \n",
    "                                             is_predict=is_predict)  \n",
    "        prompts.append(single_prompt)  # Append the generated prompt to the list\n",
    "    \n",
    "    return prompts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def prompt_preparation_pipeline(train_dataframe: pd.DataFrame,\n",
    "                                train_prompt_template: str,\n",
    "                                train_system_prompt: str,\n",
    "                                test_dataframe: pd.DataFrame,\n",
    "                                test_prompt_template: str,\n",
    "                                test_system_prompt: str,\n",
    "                                ):\n",
    "    train_prompts = create_prompts(dataframe=train_dataframe,\n",
    "                                   prompt_template=train_prompt_template,\n",
    "                                   system_prompt=train_system_prompt,\n",
    "                                   is_predict=False)\n",
    "    \n",
    "    test_prompts = create_prompts(dataframe=test_dataframe,\n",
    "                                   prompt_template=test_prompt_template,\n",
    "                                   system_prompt=test_system_prompt,\n",
    "                                   is_predict=True)\n",
    "    \n",
    "    return train_prompts, test_prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Prompts = 142.\n",
      "Test Prompts = 8.\n"
     ]
    }
   ],
   "source": [
    "train_prompts, test_prompts = prompt_preparation_pipeline(train_dataframe=train_dataframe,\n",
    "                                                          train_prompt_template=TRAINING_PROMPT_TEMPLATE,\n",
    "                                                          train_system_prompt=TRAINING_SYSTEM_PROMPT,\n",
    "                                                          test_dataframe=test_dataframe,\n",
    "                                                          test_prompt_template=PREDICTION_PROMPT_TEMPLATE,\n",
    "                                                          test_system_prompt=PREDICTION_SYSTEM_PROMPT\n",
    "                                                        )\n",
    "print(f\"Train Prompts = {len(train_prompts)}.\")\n",
    "print(f\"Test Prompts = {len(test_prompts)}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><b>Model Preparation</b></h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from typing import Any\n",
    "\n",
    "def get_bnb_config(load_in_4bit: bool = True,\n",
    "                   bnb_4bit_use_double_quant: bool = True,\n",
    "                   bnb_4bit_quant_type: str = \"nf4\",\n",
    "                   bnb_4bit_compute_dtype: Any = torch.bfloat16\n",
    "                   ) -> BitsAndBytesConfig:\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=load_in_4bit,# Whether to load model in 4-bit precision\n",
    "        bnb_4bit_use_double_quant=bnb_4bit_use_double_quant, # Whether to use double quantization\n",
    "        bnb_4bit_quant_type=bnb_4bit_quant_type,# The quantization type (e.g., \"nf4\")\n",
    "        bnb_4bit_compute_dtype=bnb_4bit_compute_dtype# The compute dtype (e.g., torch.bfloat16, torch.float16)\n",
    "    )\n",
    "    return bnb_config\n",
    "\n",
    "def get_model(model_path: str,\n",
    "              bnb_config: BitsAndBytesConfig,\n",
    "              device:str):\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_path,\n",
    "                                                 quantization_config=bnb_config,\n",
    "                                                 device_map = \"auto\")\n",
    "    return model\n",
    "\n",
    "\n",
    "def get_tokenizer(model_path: str, device: str) -> tuple:\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.padding_side = \"right\"\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_preparation_pipeline(model_path:str,\n",
    "                               device:str\n",
    "                                ):\n",
    "    \n",
    "    bnb_config = get_bnb_config()\n",
    "    model = get_model(model_path=model_path,\n",
    "                      bnb_config=bnb_config,\n",
    "                      device=device)\n",
    "    tokenizer = get_tokenizer(model_path=model_path,\n",
    "                              device=device)\n",
    "    \n",
    "    return model, tokenizer\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "223d7911d0ff465e8bfe66be837522ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got Training Model.\n",
      "Got Training Tokenizer.\n"
     ]
    }
   ],
   "source": [
    "training_model, training_tokenizer = model_preparation_pipeline(model_path=MODEL_PATH,\n",
    "                                                                device=DEVICE)\n",
    "print(f\"Got Training Model.\")\n",
    "print(f\"Got Training Tokenizer.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><b>Fine Tune</b></h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from transformers import TrainingArguments\n",
    "from trl import SFTTrainer\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "\n",
    "def get_lora_config(r: int = 16,\n",
    "                    lora_alpha: int = 64,\n",
    "                    target_modules: list[str] = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    "                    lora_dropout: float = 0.1,\n",
    "                    bias: str = \"none\",\n",
    "                    task_type: str = \"CAUSAL_LM\"\n",
    "                    ) -> LoraConfig:\n",
    "    lora_config = LoraConfig(\n",
    "        r=r,# The rank of the low-rank decomposition\n",
    "        lora_alpha=lora_alpha,# Scaling factor for the low-rank matrix\n",
    "        target_modules=target_modules,# Target modules (e.g., LLaMA-specific layers)\n",
    "        lora_dropout=lora_dropout,# Dropout rate for the low-rank layers\n",
    "        bias=bias,# Bias term (\"none\", \"all\", or \"lora_only\")\n",
    "        task_type=task_type # Task type (e.g., \"CAUSAL_LM\")\n",
    "    )\n",
    "    return lora_config\n",
    "\n",
    "def apply_lora(model,lora_config):\n",
    "    model.gradient_checkpointing_enable()\n",
    "    model = prepare_model_for_kbit_training(model)\n",
    "    model = get_peft_model(model, lora_config)\n",
    "    return model\n",
    "\n",
    "\n",
    "def get_training_args(\n",
    "    output_dir: str,\n",
    "    per_device_train_batch_size: int,\n",
    "    gradient_accumulation_steps: int,\n",
    "    logging_steps: int,\n",
    "    learning_rate: float,\n",
    "    num_train_epochs: int,\n",
    "    eval_steps: int,\n",
    "    seed: int = 42,\n",
    "    optim: str = \"paged_adamw_32bit\",\n",
    "    fp16: bool = True,\n",
    "    weight_decay: float = 0.01,\n",
    "    max_grad_norm: float = 0.3,\n",
    "    evaluation_strategy: str = \"steps\",\n",
    "    warmup_ratio: float = 0.05,\n",
    "    save_strategy: str = \"epoch\",\n",
    "    group_by_length: bool = True,\n",
    "    lr_scheduler_type: str = \"cosine\",\n",
    "    push_to_hub: bool = True,\n",
    ") -> TrainingArguments:\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        per_device_train_batch_size=per_device_train_batch_size,\n",
    "        gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "        optim=optim,\n",
    "        logging_steps=logging_steps,\n",
    "        learning_rate=learning_rate,\n",
    "        fp16=fp16,\n",
    "        weight_decay=weight_decay,\n",
    "        max_grad_norm=max_grad_norm,\n",
    "        num_train_epochs=num_train_epochs,\n",
    "        evaluation_strategy=evaluation_strategy,\n",
    "        eval_steps=eval_steps,\n",
    "        warmup_ratio=warmup_ratio,\n",
    "        save_strategy=save_strategy,\n",
    "        group_by_length=group_by_length,\n",
    "        lr_scheduler_type=lr_scheduler_type,\n",
    "        seed=seed,\n",
    "        push_to_hub=push_to_hub,\n",
    "    )\n",
    "    return training_args\n",
    "\n",
    "\n",
    "def train_model(model,\n",
    "                tokenizer,\n",
    "                lora_config,\n",
    "                training_args,\n",
    "                train_prompts,\n",
    "                val_prompts,\n",
    "                max_seq_length: int = 100):\n",
    "    \n",
    "    train_dataset = Dataset.from_pandas(pd.DataFrame({\"text\": train_prompts}))\n",
    "    val_dataset = Dataset.from_pandas(pd.DataFrame({\"text\": val_prompts}))\n",
    "    # Initialize the trainer\n",
    "    trainer = SFTTrainer(\n",
    "        model=model,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "        peft_config=lora_config,\n",
    "        dataset_text_field=\"text\",\n",
    "        max_seq_length=max_seq_length,\n",
    "        tokenizer=tokenizer,\n",
    "        args=training_args,\n",
    "    )\n",
    "    \n",
    "    # Train the model\n",
    "    trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def finetune_pipeline(model,\n",
    "                      tokenizer,\n",
    "                      train_prompts,\n",
    "                      val_prompts,\n",
    "                      finetuned_model_dir: str,\n",
    "                      batch_size: int,\n",
    "                      grad_accum_steps: int,\n",
    "                      logging_steps: int,\n",
    "                      learning_rate: float,\n",
    "                      num_epochs: int,\n",
    "                      eval_steps: int,\n",
    "\n",
    "                      max_seq_length: int = 100,\n",
    "                      optim: str = \"paged_adamw_32bit\",\n",
    "                      fp16: bool = True,\n",
    "                      weight_decay: float = 0.01,\n",
    "                      max_grad_norm: float = 0.3,\n",
    "                      evaluation_strategy: str = \"steps\",\n",
    "                      warmup_ratio: float = 0.05,\n",
    "                      save_strategy: str = \"epoch\",\n",
    "                      group_by_length: bool = True,\n",
    "                      lr_scheduler_type: str = \"cosine\",\n",
    "                      push_to_hub: bool = True,):\n",
    "    \n",
    "\n",
    "    lora_config = get_lora_config()\n",
    "    lora_applied_model = apply_lora(model=model, lora_config=lora_config)\n",
    "\n",
    "    trainable_params = sum(p.numel() for p in lora_applied_model.parameters() if p.requires_grad)\n",
    "    total_params = sum(p.numel() for p in lora_applied_model.parameters())\n",
    "    print(f\"Trainable params: {trainable_params} || Total params: {total_params} || Trainable%: {100 * trainable_params / total_params:.2f}%\")\n",
    "\n",
    "    training_args = get_training_args(output_dir=finetuned_model_dir,\n",
    "                                      per_device_train_batch_size=batch_size,\n",
    "                                      gradient_accumulation_steps=grad_accum_steps,\n",
    "                                      logging_steps=logging_steps,\n",
    "                                      learning_rate=learning_rate,\n",
    "                                      num_train_epochs=num_epochs,\n",
    "                                      eval_steps=eval_steps,\n",
    "                                      optim=optim,\n",
    "                                      fp16=fp16,\n",
    "                                      weight_decay=weight_decay,\n",
    "                                      max_grad_norm=max_grad_norm,\n",
    "                                      evaluation_strategy=evaluation_strategy,\n",
    "                                      warmup_ratio=warmup_ratio,\n",
    "                                      save_strategy=save_strategy,\n",
    "                                      group_by_length=group_by_length,\n",
    "                                      lr_scheduler_type=lr_scheduler_type,\n",
    "                                      push_to_hub=push_to_hub)\n",
    "    train_model(model=lora_applied_model,\n",
    "                tokenizer=tokenizer,\n",
    "                lora_config=lora_config,\n",
    "                training_args=training_args,\n",
    "                train_prompts=train_prompts,\n",
    "                val_prompts=val_prompts,\n",
    "                max_seq_length=max_seq_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable params: 16777216 || Total params: 3517190144 || Trainable%: 0.48%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9072c695b7d14be3b951a2a17f869b6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/142 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc2a4e3b4cca4f55841a7cd7becc9403",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/8 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "821001346fda4f2fa602840487a2dbb2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/45 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.4521, 'grad_norm': 3.1178479194641113, 'learning_rate': 1.866025403784439e-05, 'epoch': 1.11}\n",
      "{'loss': 2.7338, 'grad_norm': 2.581815242767334, 'learning_rate': 1.2947551744109044e-05, 'epoch': 2.22}\n",
      "{'loss': 2.1983, 'grad_norm': 3.1041548252105713, 'learning_rate': 5.66116260882442e-06, 'epoch': 3.33}\n",
      "{'loss': 1.8761, 'grad_norm': 3.2031314373016357, 'learning_rate': 6.912625135579587e-07, 'epoch': 4.44}\n",
      "{'train_runtime': 359.7158, 'train_samples_per_second': 1.974, 'train_steps_per_second': 0.125, 'train_loss': 2.4802604887220596, 'epoch': 5.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1858"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finetune_pipeline(model=training_model,\n",
    "                  tokenizer=training_tokenizer,\n",
    "                  train_prompts=train_prompts,\n",
    "                  val_prompts=test_prompts,\n",
    "                  finetuned_model_dir= FINETUNED_MODEL_PATH,\n",
    "                  batch_size= BATCH_SIZE,\n",
    "                  grad_accum_steps=GRAD_ACCUM_STEPS,\n",
    "                  logging_steps= LOGGING_STEPS,\n",
    "                  learning_rate= LEARNING_RATE,\n",
    "                  num_epochs= NUM_EPOCHS,\n",
    "                  eval_steps= EVAL_STEPS)\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><b>Generate</b><h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract_answer(generated_text):\n",
    "    # Extract the Answer Portion from the whole generated text\n",
    "    answer_start = generated_text.find(\"[/INST]\") + len(\"[/INST]\")  # Find the end of </INST> tag\n",
    "    answer = generated_text[answer_start:].strip()  # Extract everything after that position\n",
    "    return answer    \n",
    "\n",
    "\n",
    "def generate(prompt,model,tokenizer,max_new_tokens: int = 100):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "    # Generate response\n",
    "    output = model.generate(input_ids=inputs[\"input_ids\"], max_new_tokens=max_new_tokens)\n",
    "\n",
    "    # Decode the response\n",
    "    generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "    return generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generation_pipeline(prompts,\n",
    "                               model,\n",
    "                               tokenizer,\n",
    "                               max_new_tokens):\n",
    "        generated_answers = []\n",
    "        for idx, prompt in enumerate(prompts):\n",
    "                generated_text = generate(prompt=prompt,\n",
    "                                          model=model,\n",
    "                                          tokenizer=tokenizer,\n",
    "                                          max_new_tokens=max_new_tokens)\n",
    "                generated_answer = extract_answer(generated_text=generated_text)\n",
    "                generated_answers.append(generated_answer)\n",
    "        return generated_answers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "034c1d3504c64f9dadfc781cc31ec639",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got Finetuned Model.\n",
      "Got Finetuned Tokenizer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "finetuned_model, finetuned_tokenizer = model_preparation_pipeline(model_path=FINETUNED_MODEL_PATH,\n",
    "                                                                device=DEVICE)\n",
    "print(f\"Got Finetuned Model.\")\n",
    "print(f\"Got Finetuned Tokenizer.\")\n",
    "generated_answers = generation_pipeline(prompts=test_prompts,\n",
    "                                        model=finetuned_model,\n",
    "                                        tokenizer=finetuned_tokenizer,\n",
    "                                        max_new_tokens=MAX_NEW_TOKENS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><b>Evaluate</b></h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def calculate_cosine_similarity(real_answer, generated_answer):\n",
    "    # Initialize TF-IDF Vectorizer\n",
    "    vectorizer = TfidfVectorizer()\n",
    "\n",
    "    # Transform the answers into TF-IDF vectors\n",
    "    vectors = vectorizer.fit_transform([real_answer, generated_answer])\n",
    "\n",
    "    # Compute cosine similarity\n",
    "    similarity = cosine_similarity(vectors[0], vectors[1])[0][0]\n",
    "\n",
    "    return similarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def evaluation_pipeline(test_dataframe, generated_answers):\n",
    "    # Initialize a list to store the results\n",
    "    results = []\n",
    "\n",
    "    for idx, generated_answer in enumerate(generated_answers):\n",
    "        question = test_dataframe.iloc[idx, 0]\n",
    "        original_answer = test_dataframe.iloc[idx, 1]  # Assuming the original answer is in the second column\n",
    "        cos_similarity = calculate_cosine_similarity(original_answer, generated_answer)\n",
    "\n",
    "        # Append the result as a dictionary\n",
    "        results.append({\n",
    "            \"Question\": question,\n",
    "            \"Original Answer\": original_answer,\n",
    "            \"Generated Answer\": generated_answer,\n",
    "            \"Cosine Similarity\": cos_similarity\n",
    "        })\n",
    "\n",
    "    # Convert the results list to a DataFrame\n",
    "    result_df = pd.DataFrame(results)\n",
    "    return result_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Question</th>\n",
       "      <th>Original Answer</th>\n",
       "      <th>Generated Answer</th>\n",
       "      <th>Cosine Similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Does Corning have positive working capital bas...</td>\n",
       "      <td>Yes. Corning had a positive working capital am...</td>\n",
       "      <td>Based on the information provided in the conso...</td>\n",
       "      <td>0.320811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is Amazon's FY2017 days payable outstandi...</td>\n",
       "      <td>93.86</td>\n",
       "      <td>To answer the question, we need to calculate A...</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Does Paypal have positive working capital base...</td>\n",
       "      <td>Yes. Paypal has a positive working capital of ...</td>\n",
       "      <td>Based on the information provided in the conso...</td>\n",
       "      <td>0.354419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Has CVS Health paid dividends to common shareh...</td>\n",
       "      <td>Yes, CVS paid a $ 0.55 dividend per share ever...</td>\n",
       "      <td>Based on the evidence provided in the text, th...</td>\n",
       "      <td>0.306135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Is CVS Health a capital-intensive business bas...</td>\n",
       "      <td>Yes, CVS Health requires an extensive asset ba...</td>\n",
       "      <td>Based on the evidence provided in the text, th...</td>\n",
       "      <td>0.320862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Does AMD have a reasonably healthy liquidity p...</td>\n",
       "      <td>Yes. The quick ratio is 1.57, calculated as (c...</td>\n",
       "      <td>Based on the quick ratio calculated from the p...</td>\n",
       "      <td>0.383878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Is Boeing's business subject to cyclicality?</td>\n",
       "      <td>Yes, Boeing's business is subject to cyclicali...</td>\n",
       "      <td>According to the provided text, the answer to ...</td>\n",
       "      <td>0.557308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Did Ulta Beauty's wages expense as a percent o...</td>\n",
       "      <td>Wages expense as a percent of net sales increa...</td>\n",
       "      <td>Based on the evidence provided in the text, Ul...</td>\n",
       "      <td>0.399587</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Question  \\\n",
       "0  Does Corning have positive working capital bas...   \n",
       "1  What is Amazon's FY2017 days payable outstandi...   \n",
       "2  Does Paypal have positive working capital base...   \n",
       "3  Has CVS Health paid dividends to common shareh...   \n",
       "4  Is CVS Health a capital-intensive business bas...   \n",
       "5  Does AMD have a reasonably healthy liquidity p...   \n",
       "6       Is Boeing's business subject to cyclicality?   \n",
       "7  Did Ulta Beauty's wages expense as a percent o...   \n",
       "\n",
       "                                     Original Answer  \\\n",
       "0  Yes. Corning had a positive working capital am...   \n",
       "1                                              93.86   \n",
       "2  Yes. Paypal has a positive working capital of ...   \n",
       "3  Yes, CVS paid a $ 0.55 dividend per share ever...   \n",
       "4  Yes, CVS Health requires an extensive asset ba...   \n",
       "5  Yes. The quick ratio is 1.57, calculated as (c...   \n",
       "6  Yes, Boeing's business is subject to cyclicali...   \n",
       "7  Wages expense as a percent of net sales increa...   \n",
       "\n",
       "                                    Generated Answer  Cosine Similarity  \n",
       "0  Based on the information provided in the conso...           0.320811  \n",
       "1  To answer the question, we need to calculate A...           0.000000  \n",
       "2  Based on the information provided in the conso...           0.354419  \n",
       "3  Based on the evidence provided in the text, th...           0.306135  \n",
       "4  Based on the evidence provided in the text, th...           0.320862  \n",
       "5  Based on the quick ratio calculated from the p...           0.383878  \n",
       "6  According to the provided text, the answer to ...           0.557308  \n",
       "7  Based on the evidence provided in the text, Ul...           0.399587  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation_result_dataframe = evaluation_pipeline(test_dataframe=test_dataframe,\n",
    "                                                  generated_answers=generated_answers)\n",
    "evaluation_result_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qna",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
